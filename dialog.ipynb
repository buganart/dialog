{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dialog.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buganart/dialog/blob/master/dialog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5hTlhspTbjX",
        "cellView": "form"
      },
      "source": [
        "# @title Setup\n",
        "# @markdown 1. Before starting please save the notebook in your drive by clicking on `File -> Save a copy in drive`\n",
        "# @markdown 2. Check GPU, should be a Tesla V100 if you want to train it as fast as possible.\n",
        "# @markdown 3. Mount google drive.\n",
        "# @markdown 4. Log in to wandb.\n",
        "\n",
        "\n",
        "!nvidia-smi -L\n",
        "import os\n",
        "\n",
        "print(f\"We have {os.cpu_count()} CPU cores.\")\n",
        "print()\n",
        "\n",
        "try:\n",
        "    from google.colab import drive, output\n",
        "\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    from IPython.display import clear_output\n",
        "\n",
        "    IN_COLAB = False\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "if IN_COLAB:\n",
        "    drive.mount(\"/content/drive/\")\n",
        "\n",
        "    if not Path(\"/content/drive/My Drive/IRCMS_GAN_collaborative_database\").exists():\n",
        "        raise RuntimeError(\n",
        "            \"Shortcut to our shared drive folder doesn't exits.\\n\\n\"\n",
        "            \"\\t1. Go to the google drive web UI\\n\"\n",
        "            '\\t2. Right click shared folder IRCMS_GAN_collaborative_database and click \"Add shortcut to Drive\"'\n",
        "        )\n",
        "\n",
        "clear = output.clear if IN_COLAB else clear_output\n",
        "\n",
        "\n",
        "def clear_on_success(msg=\"Ok!\"):\n",
        "    if _exit_code == 0:\n",
        "        clear()\n",
        "        print(msg)\n",
        "\n",
        "\n",
        "print()\n",
        "print(\"Wandb installation and login ...\")\n",
        "%pip install -q wandb\n",
        "\n",
        "wandb_drive_netrc_path = Path(\"drive/My Drive/colab/.netrc\")\n",
        "wandb_local_netrc_path = Path(\"/root/.netrc\")\n",
        "if wandb_drive_netrc_path.exists():\n",
        "    import shutil\n",
        "\n",
        "    print(\"Wandb .netrc file found, will use that to log in.\")\n",
        "    shutil.copy(wandb_drive_netrc_path, wandb_local_netrc_path)\n",
        "else:\n",
        "    print(\n",
        "        f\"Wandb config not found at {wandb_drive_netrc_path}.\\n\"\n",
        "        f\"Using manual login.\\n\\n\"\n",
        "        f\"To use auto login in the future, finish the manual login first and then run:\\n\\n\"\n",
        "        f\"\\t!mkdir -p '{wandb_drive_netrc_path.parent}'\\n\"\n",
        "        f\"\\t!cp {wandb_local_netrc_path} '{wandb_drive_netrc_path}'\\n\\n\"\n",
        "        f\"Then that file will be used to login next time.\\n\"\n",
        "    )\n",
        "\n",
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2nGSKlz8xJU",
        "cellView": "form"
      },
      "source": [
        "#@title Configuration\n",
        "\n",
        "# Fill in the configuration then Then, select `Runtime` and `Run all` then let it ride!\n",
        "\n",
        "#@markdown #### Training\n",
        "drive_dirs = list(Path('/content/drive').glob('*'))\n",
        "if not drive_dirs:\n",
        "    raise RuntimeError(\"Drive not found. Is it mounted?\")\n",
        "drive = drive_dirs[0]\n",
        "print(f\"Google drive at {drive}\")    \n",
        "\n",
        "drive_audio_db_root = drive\n",
        "collaborative_database = drive / \"IRCMS_GAN_collaborative_database\"\n",
        "experiment_dir = collaborative_database / \"Experiments\" / \"colab-dialog\" \n",
        "experiment_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#@markdown The path of the text directory you would like to work with\n",
        "text_dir = \"/content/drive/MyDrive/IRCMS_GAN_collaborative_database/Research/Mathis/dialog/rick-and-morty\" #@param {type:\"string\"}\n",
        "text_dir = Path(text_dir)\n",
        "if not text_dir.exists():\n",
        "    raise RuntimeError(f\"The text_dir {text_dir} does not exist.\")\n",
        "\n",
        "#@markdown Name of pre-trained model (only tested `-small` so far).\n",
        "pretrained_model = \"microsoft/DialoGPT-small\" #@param [\"\\\"microsoft/DialoGPT-small\\\"\", \"\\\"microsoft/DialoGPT-medium\\\"\", \"\\\"microsoft/DialoGPT-large\\\"\"] {type:\"raw\", allow-input: true}\n",
        "\n",
        "# #@markdown [Optional] ID of wandb run to resume.\n",
        "# #resume_run_id = \"\" #@param {type: \"string\"}\n",
        "\n",
        "def check_wandb_id(run_id):\n",
        "    import re\n",
        "    if run_id and not re.match(r\"^[\\da-z]{8}$\", run_id):\n",
        "        raise RuntimeError(\n",
        "            \"Run ID needs to be 8 characters long and contain only letters a-z and digits.\\n\"\n",
        "            f\"Got \\\"{run_id}\\\"\"\n",
        "        )\n",
        "\n",
        "# check_wandb_id(resume_run_id)\n",
        "\n",
        "config = dict(\n",
        "    text_dir=text_dir,\n",
        "    experiment_dir=experiment_dir,\n",
        "    pretrained_model=pretrained_model,\n",
        "    # resume_run_id=resume_run_id,\n",
        ")\n",
        "for k,v in config.items():\n",
        "    print(f\"=> {k:20}: {v}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjxLUZlFBKyP",
        "cellView": "form"
      },
      "source": [
        "#@title Clone `buganart/dialog` repo.\n",
        "if IN_COLAB:\n",
        "    !git clone https://github.com/buganart/dialog\n",
        "    clear_on_success(\"Repo cloned!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chU4MVd1BSct",
        "cellView": "form"
      },
      "source": [
        "#@title Install dependencies\n",
        "\n",
        "%pip install -e ./dialog\n",
        "import site\n",
        "site.main()\n",
        "clear_on_success(\"Dependencies installed!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOQSMDxHU4UC",
        "cellView": "form"
      },
      "source": [
        "#@title Finetune\n",
        "env_vars = dict(\n",
        "    WANDB_ENTITY=\"bugan\",\n",
        "    WANDB_PROJECT=\"dialog\",\n",
        ")\n",
        "for name, value in env_vars.items():\n",
        "    os.environ[name] = value\n",
        "\n",
        "from dialog import finetune\n",
        "\n",
        "trainer = finetune.train(\n",
        "    text_dir=text_dir, \n",
        "    save_dir=experiment_dir, \n",
        "    pretrained_model=pretrained_model,\n",
        ")\n",
        "\n",
        "import wandb\n",
        "wandb.join()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "wPhdExhpGlZx"
      },
      "source": [
        "#@title Generate text.\n",
        "import torch\n",
        "from dialog import generate\n",
        "\n",
        "#@markdown Prefix for text generation.\n",
        "prefix = \"Hurry up Morty!\" #@param {type:\"string\"}\n",
        "#@markdown Number of responses to genrate.\n",
        "steps =  20#@param {type:\"integer\"}\n",
        "\n",
        "#@markdown [Optional] Checkpoint path with trained model. **No need to specify if you just trained the model**.\n",
        "checkpoint_dir = \"\" #@param {type:\"string\"}\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if checkpoint_dir:\n",
        "    print(f\"Loading models from checkpoint {checkpoint_dir}\")\n",
        "    model = generate.load_model(checkpoint_dir).to(device)\n",
        "    tokenizer = generate.load_tokenizer(checkpoint_dir)\n",
        "else:\n",
        "    model = trainer.model.to(device)\n",
        "    tokenizer = trainer.tokenizer\n",
        "\n",
        "generate.generate(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        "    prefix=prefix,\n",
        "    steps=steps,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}